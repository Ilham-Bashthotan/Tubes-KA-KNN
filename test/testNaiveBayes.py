# -*- coding: utf-8 -*-
"""KA_Naive Bayes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uaSl5YbtjHiSz4V9pkFE2rUvdQIUGEmY
"""

import numpy as np
import pandas as pd

"""# FUNGSI-FUNGSI

FUngsi untuk Model Naive Bayes
"""

def train_test_split_stratified(X, y, test_size=0.2, random_state=42):

    """
    Split data menjadi train dan test set dengan stratifikasi.
    I.S. X: fitur, y: target
    F.S : X_train, X_test, y_train, y_test
    """

    np.random.seed(random_state)
    combined = X.copy()
    combined['_target_'] = y.values

    class_0 = combined[combined['_target_'] == 0]
    class_1 = combined[combined['_target_'] == 1]


    class_0_shuffled = class_0.sample(frac=1, random_state=random_state).reset_index(drop=True)
    class_1_shuffled = class_1.sample(frac=1, random_state=random_state).reset_index(drop=True)

    test_count_0 = int(len(class_0) * test_size)
    test_count_1 = int(len(class_1) * test_size)

    test_0 = class_0_shuffled[:test_count_0]
    train_0 = class_0_shuffled[test_count_0:]
    test_1 = class_1_shuffled[:test_count_1]
    train_1 = class_1_shuffled[test_count_1:]

    train_combined = pd.concat([train_0, train_1], ignore_index=True)
    test_combined = pd.concat([test_0, test_1], ignore_index=True)

    train_combined = train_combined.sample(frac=1, random_state=random_state).reset_index(drop=True)
    test_combined = test_combined.sample(frac=1, random_state=random_state).reset_index(drop=True)

    X_train = train_combined.drop('_target_', axis=1)
    y_train = train_combined['_target_']
    X_test = test_combined.drop('_target_', axis=1)
    y_test = test_combined['_target_']

    return X_train, X_test, y_train, y_test

def standardize(X_train, X_test):
    """
    Standardisasi Z-score.
    I.S. X_train, X_test: fitur
    F.S : X_train_scaled, X_test_scaled
    """
    mean = X_train.mean()
    std = X_train.std()
    X_train_scaled = (X_train - mean) / std
    X_test_scaled = (X_test - mean) / std
    return X_train_scaled, X_test_scaled

def calculate_distances_vectorized(X_test_point, X_train, metric='euclidean'):
    """
    Hitung jarak dari 1 test point ke semua training points (vectorized).
    I.S. X_test_point: 1 data point, X_train: semua training points
    F.S : array jarak
    """
    if metric == 'euclidean':
        return np.sqrt(np.sum((X_train - X_test_point) ** 2, axis=1))
    elif metric == 'manhattan':
        return np.sum(np.abs(X_train - X_test_point), axis=1)
    else:
        raise ValueError(f"Metric {metric} tidak didukung")

"""Model Naive Bayes"""

# --- 1. Fungsi Pelatihan (Fit) ---

def manual_gaussian_nb_fit(X, y):
    """
    Melatih model Gaussian Naive Bayes dengan menghitung mean, variance, dan prior.

    Args:
        X (np.array): Data pelatihan (fitur).
        y (np.array): Label kelas pelatihan.

    Returns:
        dict: Parameter model yang telah dilatih.
    """
    model_params = {}
    model_params['classes'] = np.unique(y)
    model_params['means'] = {}
    model_params['vars'] = {}
    model_params['priors'] = {}

    n_samples = len(X)

    for c in model_params['classes']:
        X_c = X[y == c]

        # Simpan mean dan variance tiap fitur
        model_params['means'][c] = X_c.mean(axis=0)
        # Tambahkan epsilon kecil untuk menghindari pembagian dengan nol saat menghitung PDF
        model_params['vars'][c] = X_c.var(axis=0) + 1e-9

        # Prior: P(class)
        model_params['priors'][c] = len(X_c) / n_samples

    return model_params

# --- 2. Fungsi Kepadatan Probabilitas Gaussian (PDF) ---

def gaussian_pdf(x, mean, var):
    """
    Menghitung Probability Density Function (PDF) untuk distribusi Gaussian.

    Args:
        x (float/np.array): Nilai yang akan dievaluasi.
        mean (float/np.array): Rata-rata distribusi.
        var (float/np.array): Variansi distribusi.

    Returns:
        float/np.array: Nilai kepadatan probabilitas.
    """
    numerator = np.exp(- (x - mean)**2 / (2 * var))
    denominator = np.sqrt(2 * np.pi * var)
    return numerator / denominator

#

# --- 3. Fungsi Prediksi untuk Satu Sampel ---

def manual_gaussian_nb_predict_single(x, model_params):
    """
    Memprediksi label kelas untuk satu sampel (vektor fitur).

    Args:
        x (np.array): Sampel fitur tunggal.
        model_params (dict): Parameter model hasil pelatihan.

    Returns:
        any: Kelas yang diprediksi.
    """
    posteriors = {}

    for c in model_params['classes']:
        prior = np.log(model_params['priors'][c])

        # Hitung log-likelihood (Probabilitas Kondisional)
        # Log(P(x|c)) = Log(Product(P(x_i|c))) = Sum(Log(P(x_i|c)))
        class_conditional = np.sum(
            np.log(gaussian_pdf(x, model_params['means'][c], model_params['vars'][c]))
        )

        # Hitung Log-Posterior: Log(P(c|x)) ~ Log(P(c)) + Log(P(x|c))
        posteriors[c] = prior + class_conditional

    # Pilih kelas dengan log-posterior tertinggi
    return max(posteriors, key=posteriors.get)

# --- 4. Fungsi Prediksi untuk Banyak Sampel ---

def manual_gaussian_nb_predict(X, model_params):
    """
    Memprediksi label kelas untuk beberapa sampel.

    Args:
        X (np.array): Data fitur.
        model_params (dict): Parameter model hasil pelatihan.

    Returns:
        np.array: Array label kelas yang diprediksi.
    """
    # Lakukan prediksi untuk setiap baris di X
    return np.array([manual_gaussian_nb_predict_single(x, model_params) for x in X])

"""Metrik Evaluasi"""

def accuracy(y_true, y_pred):
    """
    Hitung akurasi.
    I.S. y_true: label asli, y_pred: label prediksi
    F.S : akurasi (float)
    """
    return np.sum(y_true == y_pred) / len(y_true)

def confusion_matrix(y_true, y_pred):
    """
    Hitung confusion matrix.
    I.S. y_true: label asli, y_pred: label prediksi
    F.S : confusion matrix (2D array)
    """
    classes = np.unique(np.concatenate([y_true, y_pred]))
    cm = np.zeros((len(classes), len(classes)), dtype=int)
    for true, pred in zip(y_true, y_pred):
        true_idx = np.where(classes == true)[0][0]
        pred_idx = np.where(classes == pred)[0][0]
        cm[true_idx, pred_idx] += 1
    return cm

"""MAIN PROGRAM"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("adityakadiwal/water-potability")

print("Path to dataset files:", path)

import os

# Construct the full path to the CSV file
csv_file_path = os.path.join(path, 'water_potability.csv')
df = pd.read_csv(csv_file_path)
df

"""Preprocessing: Missing Value Problem"""

# 1. Inspeksi Data Awal
print(f"Data awal (sebelum pembersihan): {df.shape}")
print("-" * 30)

# 2. Inspeksi Missing Values Awal
print("Jumlah Missing Values (per kolom, sebelum dropna):")
print(df.isnull().sum())
print("-" * 30)

# 3. Penghapusan Missing Values (Koreksi: Baris dropna diaktifkan)
df = df.dropna()

# 4. Inspeksi Data Setelah Penghapusan
print(f"Data setelah dropna: {df.shape}")
print("Verifikasi Missing Values (setelah dropna):")
print(df.isnull().sum())
print("-" * 30)

# 5. Inspeksi Duplikat
print("Jumlah data yang duplikat (sebelum penghapusan duplikat):", df.duplicated().sum())
df = df.drop_duplicates()
print("Jumlah data setelah penghapusan duplikat:", df.shape)

"""EDA"""

# Ikutin yang lain

"""correlation between features"""

import matplotlib.pyplot as plt
import seaborn as sns

# ------------------------------------------
#  CORRELATION HEATMAP
# ------------------------------------------
corr = df.corr()
plt.figure(figsize=(10,7))
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

"""Distribution of Features"""

df.head()

print(df_copy.shape)
print(df.shape)

"""Split Data"""

X = df.drop(['Potability'], axis=1)
y = df['Potability']

X_train, X_test, y_train, y_test = train_test_split_stratified(
    X,
    y,
    test_size=0.2,
    random_state=42,
)

# --- 3. Verifikasi Hasil Split ---

print("--- Ukuran Data Setelah Split ---")
print(f"Total Data: {len(df)} baris")
print("-" * 30)

print(f"X_train (Fitur Pelatihan): {X_train.shape}")
print(f"X_test (Fitur Pengujian): {X_test.shape}")
print(f"y_train (Target Pelatihan): {y_train.shape}")
print(f"y_test (Target Pengujian): {y_test.shape}")
print("-" * 30)

# Verifikasi Proporsi Kelas (contoh pentingnya stratify)
print("Proporsi Kelas di y_test:")
print(y_test.value_counts(normalize=True).round(3))

"""Split data df encoded

Penghapusan Outlier
"""

# Hitung IQR bounds dari TRAIN SET saja (mencegah data leakage)
outlier_bounds = {}
for col in X_train.columns:
    Q1 = X_train[col].quantile(0.25)
    Q3 = X_train[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    outlier_bounds[col] = (lower, upper)

#Terapkan bounds ke train dan test set
X_train_outlier_removed = X_train.copy()
X_test_outlier_removed = X_test.copy()

for col in X_train.columns:
    lower, upper = outlier_bounds[col]
    X_train_outlier_removed[col] = X_train_outlier_removed[col].clip(lower, upper)
    X_test_outlier_removed[col] = X_test_outlier_removed[col].clip(lower, upper)

print("Outlier capping selesai (menggunakan bounds dari train set)")

X_train_prep, X_test_prep = standardize(X_train_outlier_removed, X_test_outlier_removed)

"""Model: Naive Bayes

Data non encoded
"""

X_train_np = X_train_prep.values
y_train_np = y_train.values
X_test_np = X_test_prep.values
y_test_np = y_test.values

trained_model = manual_gaussian_nb_fit(X_train_np, y_train_np)

y_pred = manual_gaussian_nb_predict(X_test_np, trained_model)

"""Data Encoded

Evaluasi Metrik
"""

from sklearn.metrics import accuracy_score, precision_score, f1_score, ConfusionMatrixDisplay, classification_report
import matplotlib.pyplot as plt

# Calculate evaluation metrics
accuracy_model = accuracy_score(y_test_np, y_pred)
precision_model = precision_score(y_test_np, y_pred, average='binary')
f1_model = f1_score(y_test_np, y_pred, average='binary')

# 1. Hitung metrik evaluasi
accuracy_model = accuracy_score(y_test_np, y_pred)
precision_model = precision_score(y_test_np, y_pred, average='binary', zero_division=0)
f1_model = f1_score(y_test_np, y_pred, average='binary', zero_division=0)

# 2. Cetak Ringkasan Metrik Utama
print("=" * 40)
print("     HASIL EVALUASI MODEL GAUSSIAN NB     ")
print("=" * 40)
print(f"| {'Accuracy':<15}: {accuracy_model:.4f} |")
print(f"| {'Precision':<15}: {precision_model:.4f} |")
print(f"| {'F1-Score':<15}: {f1_model:.4f} |")
print("=" * 40)
print("\n")

# 3. Cetak Classification Report (Output Paling Rapi)
print("--- Classification Report (Detail Metrik per Kelas) ---")
# Classification report memberikan Precision, Recall, dan F1-score untuk Kelas 0 dan Kelas 1
print(classification_report(y_test_np, y_pred, target_names=['0 (Tidak Layak)', '1 (Layak)'], zero_division=0))
print("\n")

# Calculate the confusion matrix using the custom function
cm = confusion_matrix(y_test_np, y_pred)

# Visualisasi (Tetap diperlukan agar mudah diinterpretasikan)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])
# Atur ukuran figure agar visualisasi lebih besar
plt.figure(figsize=(6, 6))
disp.plot(cmap=plt.cm.Blues, values_format='d', ax=plt.gca())
plt.title("Confusion Matrix")
plt.show()

"""Eksperimen Encoding Data"""

import numpy as np
import pandas as pd

# Asumsi Anda memiliki DataFrame bernama 'df' yang sudah dimuat

def encode_water_quality(df_input):
    """
    Melakukan Binning (Diskretisasi) pada fitur numerik berdasarkan standar baku mutu (WHO, EPA)
    untuk membuat fitur kategorikal/biner baru.
    """
    df = df_input.copy()

    print("Melakukan Binning pada Fitur Numerik...")

    # --- 1. pH Value (Biner: Aman vs. Berbahaya) ---
    # WHO: 6.5 - 8.5
    df['ph'] = np.where(
        (df['ph'] >= 6.5) & (df['ph'] <= 8.5),
        'Aman',
        'Berbahaya'
    )

    # --- 2. Hardness (Ordinal: Menggunakan batas standar umum) ---
    # Hardness umumnya diklasifikasikan (misal USGS standard):
    # Lunak: < 60; Sedang: 60-120; Keras: 120-180; Sangat Keras: > 180
    bins_hardness = [0, 60, 120, 180, np.inf]
    labels_hardness = ['Lunak', 'Sedang', 'Keras', 'Sangat_Keras']
    df['Hardness'] = pd.cut(
        df['Hardness'],
        bins=bins_hardness,
        labels=labels_hardness,
        right=False,  # Batas bawah inklusif
        include_lowest=True
    )

    # --- 3. Solids (TDS) (Ordinal: Berdasarkan batas Desirable dan Max) ---
    # Desirable: <= 500 mg/L; Max: <= 1000 mg/L
    bins_tds = [0, 500, 1000, np.inf]
    labels_tds = ['Ideal', 'Batas_Toleransi', 'Diatas_Maksimum']
    df['Solids'] = pd.cut(
        df['Solids'], # Kolom Solids diasumsikan berisi TDS
        bins=bins_tds,
        labels=labels_tds,
        right=True
    )

    # --- 4. Chloramines (Biner: Aman vs. Berlebihan) ---
    # Batas Aman: <= 4 mg/L
    df['Chloramines'] = np.where(
        df['Chloramines'] <= 4,
        'Aman',
        'Berlebihan'
    )

    # --- 5. Sulfate (Biner: Menggunakan batas umum air minum 250 mg/L) ---
    # Karena tidak ada batas tegas WHO, kita gunakan batas umum sekunder (250 mg/L)
    # Ini adalah asumsi jika tidak ada data batas yang diberikan.
    df['Sulfate'] = np.where(
        df['Sulfate'] <= 250,
        'Aman',
        'Tinggi'
    )

    # --- 6. Conductivity (Biner: Aman vs. Berlebihan) ---
    # WHO: tidak boleh melebihi 400 Î¼S/cm
    df['Conductivity'] = np.where(
        df['Conductivity'] <= 400,
        'Aman',
        'Berlebihan'
    )

    # --- 7. Organic_carbon (Biner: Aman vs. Berlebihan) ---
    # US EPA Drinking Water: < 2 mg/L
    df['Organic_carbon'] = np.where(
        df['Organic_carbon'] < 2,
        'Aman',
        'Tinggi'
    )

    # --- 8. Trihalomethanes (THMs) (Biner: Aman vs. Berlebihan) ---
    # Batas Aman: <= 80 ppm (mg/L)
    df['Trihalomethanes'] = np.where(
        df['Trihalomethanes'] <= 80,
        'Aman',
        'Berlebihan'
    )

    # --- 9. Turbidity (Biner: Aman vs. Berlebihan) ---
    # WHO: < 5.00 NTU
    df['Turbidity'] = np.where(
        df['Turbidity'] < 5.00,
        'Aman',
        'Tinggi'
    )

    # --- 10. Potability ---
    # Ini sudah biner (1/0), tetapi kita ubah ke label string untuk konsistensi
    df['Potability'] = df['Potability'].map({1: 'Potable', 0: 'Not_Potable'})


    # Hapus kolom numerik asli jika Anda hanya ingin menggunakan fitur kategorikal
    # dan hapus kolom target Potability yang asli
    # cols_to_drop = ['pH', 'Hardness', 'Solids', 'Chloramines', 'Sulfate',
    #                 'Conductivity', 'Organic_carbon', 'Trihalomethanes',
    #                 'Turbidity']

    # df_encoded = df.drop(columns=cols_to_drop, errors='ignore')

    print("Encoding Selesai. Fitur baru telah dibuat.")
    return df

# Contoh Penggunaan:
# df_encoded = encode_water_quality(df)
# print(df_encoded.head())

df_copy = df.copy()
df_copy = encode_water_quality(df_copy)
df_copy

import pandas as pd

# Asumsi df_copy adalah DataFrame yang berisi data hasil encoding Anda

print("--- NILAI UNIK DI SETIAP KOLOM df_copy ---")
print("-" * 40)

for column in df_copy.columns:
    # 1. Gunakan value_counts() untuk melihat nilai unik dan frekuensinya
    # Ini sangat berguna untuk melihat apakah ada kategori yang sangat jarang atau NaN.
    unique_counts = df_copy[column].value_counts(dropna=False)

    print(f"\nKolom: {column}")
    print(f"Total Nilai Unik: {len(unique_counts)}")

    # Cetak hasil hitungan (limit 10 agar output tidak terlalu panjang)
    print("Top Kategori dan Jumlah:")
    print(unique_counts.head(10))

    print("-" * 40)

from sklearn.model_selection import train_test_split

X = df_copy.drop(['Potability'], axis=1)
y = df_copy['Potability'].map({'Potable': 1, 'Not_Potable': 0})

X_train_encoded, X_test_encoded, y_train_encoded, y_test_encoded = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
)

# --- 3. Verifikasi Hasil Split ---

print("--- Ukuran Data Setelah Split ---")
print(f"Total Data: {len(df)} baris")
print("-" * 30)

print(f"X_train (Fitur Pelatihan): {X_train_encoded.shape}")
print(f"X_test (Fitur Pengujian): {X_test_encoded.shape}")
print(f"y_train (Target Pelatihan): {y_train_encoded.shape}")
print(f"y_test (Target Pengujian): {y_test_encoded.shape}")
print("-" * 30)

# Verifikasi Proporsi Kelas (contoh pentingnya stratify)
print("Proporsi Kelas di y_test:")
print(y_test_encoded.value_counts(normalize=True).round(3))

# KOREKSI NAMA VARIABEL: Gunakan nama variabel split yang benar: X_train, X_test
print(f"X_train (Fitur Pelatihan): {X_train_encoded.shape}")
print(f"X_test (Fitur Pengujian): {X_test_encoded.shape}")
print(f"y_train (Target Pelatihan): {y_train_encoded.shape}")
print(f"y_test (Target Pengujian): {y_test_encoded.shape}")
print("-" * 30)

# Verifikasi Proporsi Kelas
print("Proporsi Kelas di y_test:")
print(y_test.value_counts(normalize=True).round(3))

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

# 1. Tentukan kolom kategorikal (semua kolom di X_train adalah kategorikal string)
categorical_features = X_train_encoded.columns

# 2. Buat ColumnTransformer untuk One-Hot Encoding (OHE)
# handle_unknown='ignore' penting agar tidak error jika ada kategori baru di test set
ct = ColumnTransformer(
    transformers=[
        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first'), categorical_features)
    ],
    remainder='passthrough' # Biarkan kolom non-kategorikal tetap ada (jika ada)
)

# 3. Terapkan (Fit) CT HANYA pada Data Latih (X_train)
ct.fit(X_train_encoded)

# 4. Transformasi Data Latih dan Uji
X_train_final = ct.transform(X_train_encoded)
X_test_final = ct.transform(X_test_encoded)

# Ubah ke DataFrame (Opsional, tapi bagus untuk melihat nama kolom)
# Dapatkan nama kolom baru setelah OHE
feature_names = ct.named_transformers_['encoder'].get_feature_names_out(categorical_features)
X_train_final_df = pd.DataFrame(X_train_final, columns=feature_names)
X_test_final_df = pd.DataFrame(X_test_final, columns=feature_names)

print("\nShape setelah One-Hot Encoding (OHE):")
print(f"X_train_final: {X_train_final_df.shape}")
print(f"X_test_final: {X_test_final_df.shape}")

# Latih Model Bernoulli Naive Bayes
from sklearn.naive_bayes import BernoulliNB

model_nb = BernoulliNB()

# KOREKSI: Gunakan X_train_final_df dan y_train yang sudah 0/1.
# y_train sudah di-map di awal, tidak perlu di-map lagi di sini.
model_nb.fit(X_train_final_df, y_train_encoded)

print("\nModel BernoulliNB berhasil dilatih!")

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import pandas as pd

# Asumsi:
# 1. model_nb sudah dilatih (dengan X_train_final_df dan y_train_encoded)
# 2. X_test_final_df (Fitur Uji yang sudah di-OHE) tersedia
# 3. y_test_encoded (Target Uji 0/1) tersedia

# --- 1. Prediksi pada Data Uji ---

# Lakukan prediksi label (0 atau 1) pada data uji
y_pred = model_nb.predict(X_test_final_df)

# Lakukan prediksi probabilitas (untuk analisis lebih lanjut, jika diperlukan)
# y_pred_proba = model_nb.predict_proba(X_test_final_df)[:, 1]


# --- 2. Evaluasi Kinerja Menggunakan Classification Report ---

print("=" * 60)
print("             HASIL EVALUASI MODEL BERNOULLI NAIVE BAYES             ")
print("=" * 60)

# Target names harus sesuai dengan nilai target 0 dan 1
target_names = ['0 (NOT Potable)', '1 (Potable)']

# Cetak Classification Report: Menampilkan Precision, Recall, F1-Score untuk setiap kelas
print("--- Classification Report (Metrik Detail per Kelas) ---")
print(classification_report(y_test_encoded, y_pred, target_names=target_names, zero_division=0))
print("-" * 60)


# --- 3. Visualisasi Confusion Matrix ---

# Dapatkan Confusion Matrix
cm = confusion_matrix(y_test_encoded, y_pred)

print("\n--- Confusion Matrix (Nilai Angka) ---")
print(cm)

# Visualisasi Confusion Matrix
disp = ConfusionMatrixDisplay(
    confusion_matrix=cm,
    display_labels=target_names
)

plt.figure(figsize=(8, 8))
# Tampilkan matriks dengan format integer ('d') dan skema warna biru
disp.plot(cmap=plt.cm.Blues, values_format='d', ax=plt.gca())
plt.title("Confusion Matrix Bernoulli Naive Bayes")
plt.show()
